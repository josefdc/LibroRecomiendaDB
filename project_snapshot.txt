# Estructura relevante del proyecto

LibroRecomienda/
├── README.md
├── alembic.ini
├── collect_snapshot.py
├── main.py
├── migrations/
│   ├── env.py
├── project_snapshot.txt
├── pyproject.toml
├── scripts/
│   ├── generate_fake_data.py
│   ├── populate_db.py
├── src/
│   ├── __init__.py
├── streamlit_app/
│   ├── app.py
├── tests/
│   ├── __init__.py

# Contenido de archivos relevantes

--------------------------------------------------------------------------------
# README.md



--------------------------------------------------------------------------------
# alembic.ini

# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to migrations/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:migrations/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = %(DATABASE_URL)s


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


--------------------------------------------------------------------------------
# collect_snapshot.py

#!/usr/bin/env python3
"""
snapshot_clean.py ― Exporta un TXT con la estructura ***concisa*** y el contenido
de los archivos realmente útiles de un proyecto.

▫️ Filtros de directorio/archivo para ruido habitual (.venv, __pycache__, *.pyc…)
▫️ Lista blanca de extensiones relevantes (.py, .toml, .md, .yml…; ajustable)
▫️ Umbral de tamaño (bytes) para saltarse binarios enormes accidentalmente
▫️ Opciones CLI para personalizar filtros sin tocar el código

Ejemplo:
    python snapshot_clean.py . -o docs/snapshot.txt --only-ext .py,.md
"""
from __future__ import annotations

import argparse
from dataclasses import dataclass
import fnmatch
import os
from pathlib import Path
from typing import Iterable, List

# ────────────────────────────────────────────────────────────────────────────────
# CONFIGURACIÓN POR DEFECTO ─ modifícala si quieres, o usa las flags CLI
# ────────────────────────────────────────────────────────────────────────────────
DEFAULT_IGNORE_DIRS = [
    ".git", ".venv", ".mypy_cache", ".pytest_cache", ".idea", ".vscode",
    "__pycache__", "*.egg-info", ".cache", ".tox", "dist", "build",
]
DEFAULT_IGNORE_FILES = [
    "*.pyc", "*.pyo", "*.so", "*.dylib", "*.log", "*.db", "*.sqlite3",
    "*.DS_Store", "*.lock", "*.zip", "*.tar.gz",
]
DEFAULT_ONLY_EXT = [
    ".py", ".toml", ".md", ".txt", ".ini", ".json", ".yaml", ".yml",
    ".sql", ".html", ".js", ".ts", ".css",
]
DEFAULT_MAX_SIZE = 1_000_000            # 1 MB

# ────────────────────────────────────────────────────────────────────────────────
# DATA STRUCTS
# ────────────────────────────────────────────────────────────────────────────────
@dataclass
class Filters:
    ignore_dirs: List[str]
    ignore_files: List[str]
    only_ext:   List[str]
    max_size:   int


# ────────────────────────────────────────────────────────────────────────────────
# HELPERS
# ────────────────────────────────────────────────────────────────────────────────
def _matches(path: Path, patterns: Iterable[str]) -> bool:
    return any(fnmatch.fnmatch(path.name, pat) for pat in patterns)

def must_ignore(path: Path, f: Filters) -> bool:
    if path.is_dir():
        return _matches(path, f.ignore_dirs)
    # archivo
    if _matches(path, f.ignore_files):
        return True
    if f.only_ext and path.suffix.lower() not in f.only_ext:
        return True
    if f.max_size and path.stat().st_size > f.max_size:
        return True
    return False


# ────────────────────────────────────────────────────────────────────────────────
# DISCOVERY
# ────────────────────────────────────────────────────────────────────────────────
def collect_files(root: Path, f: Filters) -> List[Path]:
    relevant: List[Path] = []
    for dirpath, dirnames, filenames in os.walk(root):
        # purga de subdirectorios irrelevantes in-place
        dirnames[:] = [d for d in dirnames if not must_ignore(Path(d), f)]
        for name in filenames:
            fp = Path(dirpath) / name
            if not must_ignore(fp, f):
                relevant.append(fp)
    return sorted(relevant)

def build_tree(root: Path, files: List[Path]) -> str:
    """Dibuja el árbol mostrando solo ramas que contengan archivos relevantes."""
    # Conjunto de carpetas que sí aportan algo
    keep_dirs = {root}
    for file in files:
        for parent in file.parents:
            keep_dirs.add(parent)
            if parent == root:
                break

    lines: List[str] = []
    for path in sorted(keep_dirs | set(files)):
        rel = path.relative_to(root)
        indent = "│   " * (len(rel.parts) - 1)
        prefix = "├── " if rel.parts else ""
        lines.append(f"{indent}{prefix}{path.name}{'/' if path.is_dir() else ''}")
    return "\n".join(lines)


# ────────────────────────────────────────────────────────────────────────────────
# MAIN
# ────────────────────────────────────────────────────────────────────────────────
def main() -> None:
    ap = argparse.ArgumentParser(description="Genera un snapshot limpio del proyecto.")
    ap.add_argument("root", nargs="?", default=".", help="Raíz del proyecto")
    ap.add_argument("-o", "--output", default="project_snapshot.txt", help="Archivo de salida")
    ap.add_argument("--ignore-dirs", help="Patrones coma-separados extra para ignorar")
    ap.add_argument("--ignore-files", help="Patrones coma-separados extra para ignorar")
    ap.add_argument("--only-ext", help="Extensiones permitidas (ej: .py,.md) ― vacío = todas")
    ap.add_argument("--max-size", type=int, default=DEFAULT_MAX_SIZE,
                    help=f"Tamaño máximo (bytes) de archivos a incluir (def {DEFAULT_MAX_SIZE})")
    args = ap.parse_args()

    filters = Filters(
        ignore_dirs=DEFAULT_IGNORE_DIRS + (args.ignore_dirs.split(",") if args.ignore_dirs else []),
        ignore_files=DEFAULT_IGNORE_FILES + (args.ignore_files.split(",") if args.ignore_files else []),
        only_ext=[e.lower() for e in (args.only_ext.split(",") if args.only_ext else DEFAULT_ONLY_EXT)],
        max_size=args.max_size,
    )

    root = Path(args.root).resolve()
    files = collect_files(root, filters)

    with open(args.output, "w", encoding="utf-8") as out:
        # 1. Árbol compacto
        out.write("# Estructura relevante del proyecto\n\n")
        out.write(build_tree(root, files) + "\n\n")

        # 2. Contenido de archivos
        out.write("# Contenido de archivos relevantes\n")
        for fp in files:
            rel = fp.relative_to(root)
            out.write("\n" + "-" * 80 + f"\n# {rel}\n\n")
            try:
                out.write(fp.read_text(encoding="utf-8", errors="replace") + "\n")
            except Exception as e:
                out.write(f"<<No se pudo leer este archivo: {e}>>\n")

    print(f"Snapshot listo → {args.output}")

if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------
# main.py

def main():
    print("Hello from librorecomienda!")


if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------
# migrations/env.py

import os
import sys
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool
from sqlalchemy import create_engine

from alembic import context

# Add the path to your source code
sys.path.insert(0, os.path.realpath(os.path.join(os.path.dirname(__file__), '..', 'src')))

# Import your Base and models
from librorecomienda.db.session import Base
from librorecomienda.models import *  # Import all models for migration discovery

# Import your settings
from librorecomienda.core.config import settings

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    # Use the URL from your settings instead of the config
    url = settings.DATABASE_URL
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Use create_engine with your settings instead of engine_from_config
    connectable = create_engine(settings.DATABASE_URL)

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


--------------------------------------------------------------------------------
# project_snapshot.txt

# Estructura relevante del proyecto

LibroRecomienda/
├── README.md
├── alembic.ini
├── collect_snapshot.py
├── main.py
├── migrations/
│   ├── env.py
├── project_snapshot.txt
├── pyproject.toml
├── scripts/
│   ├── generate_fake_data.py
│   ├── populate_db.py
├── src/
│   ├── __init__.py
├── streamlit_app/
│   ├── app.py
├── tests/
│   ├── __init__.py

# Contenido de archivos relevantes

--------------------------------------------------------------------------------
# README.md



--------------------------------------------------------------------------------
# alembic.ini

# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to migrations/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:migrations/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
version_path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = %(DATABASE_URL)s


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


--------------------------------------------------------------------------------
# collect_snapshot.py

#!/usr/bin/env python3
"""
snapshot_clean.py ― Exporta un TXT con la estructura ***concisa*** y el contenido
de los archivos realmente útiles de un proyecto.

▫️ Filtros de directorio/archivo para ruido habitual (.venv, __pycache__, *.pyc…)
▫️ Lista blanca de extensiones relevantes (.py, .toml, .md, .yml…; ajustable)
▫️ Umbral de tamaño (bytes) para saltarse binarios enormes accidentalmente
▫️ Opciones CLI para personalizar filtros sin tocar el código

Ejemplo:
    python snapshot_clean.py . -o docs/snapshot.txt --only-ext .py,.md
"""
from __future__ import annotations

import argparse
from dataclasses import dataclass
import fnmatch
import os
from pathlib import Path
from typing import Iterable, List

# ────────────────────────────────────────────────────────────────────────────────
# CONFIGURACIÓN POR DEFECTO ─ modifícala si quieres, o usa las flags CLI
# ────────────────────────────────────────────────────────────────────────────────
DEFAULT_IGNORE_DIRS = [
    ".git", ".venv", ".mypy_cache", ".pytest_cache", ".idea", ".vscode",
    "__pycache__", "*.egg-info", ".cache", ".tox", "dist", "build",
]
DEFAULT_IGNORE_FILES = [
    "*.pyc", "*.pyo", "*.so", "*.dylib", "*.log", "*.db", "*.sqlite3",
    "*.DS_Store", "*.lock", "*.zip", "*.tar.gz",
]
DEFAULT_ONLY_EXT = [
    ".py", ".toml", ".md", ".txt", ".ini", ".json", ".yaml", ".yml",
    ".sql", ".html", ".js", ".ts", ".css",
]
DEFAULT_MAX_SIZE = 1_000_000            # 1 MB

# ────────────────────────────────────────────────────────────────────────────────
# DATA STRUCTS
# ────────────────────────────────────────────────────────────────────────────────
@dataclass
class Filters:
    ignore_dirs: List[str]
    ignore_files: List[str]
    only_ext:   List[str]
    max_size:   int


# ────────────────────────────────────────────────────────────────────────────────
# HELPERS
# ────────────────────────────────────────────────────────────────────────────────
def _matches(path: Path, patterns: Iterable[str]) -> bool:
    return any(fnmatch.fnmatch(path.name, pat) for pat in patterns)

def must_ignore(path: Path, f: Filters) -> bool:
    if path.is_dir():
        return _matches(path, f.ignore_dirs)
    # archivo
    if _matches(path, f.ignore_files):
        return True
    if f.only_ext and path.suffix.lower() not in f.only_ext:
        return True
    if f.max_size and path.stat().st_size > f.max_size:
        return True
    return False


# ────────────────────────────────────────────────────────────────────────────────
# DISCOVERY
# ────────────────────────────────────────────────────────────────────────────────
def collect_files(root: Path, f: Filters) -> List[Path]:
    relevant: List[Path] = []
    for dirpath, dirnames, filenames in os.walk(root):
        # purga de subdirectorios irrelevantes in-place
        dirnames[:] = [d for d in dirnames if not must_ignore(Path(d), f)]
        for name in filenames:
            fp = Path(dirpath) / name
            if not must_ignore(fp, f):
                relevant.append(fp)
    return sorted(relevant)

def build_tree(root: Path, files: List[Path]) -> str:
    """Dibuja el árbol mostrando solo ramas que contengan archivos relevantes."""
    # Conjunto de carpetas que sí aportan algo
    keep_dirs = {root}
    for file in files:
        for parent in file.parents:
            keep_dirs.add(parent)
            if parent == root:
                break

    lines: List[str] = []
    for path in sorted(keep_dirs | set(files)):
        rel = path.relative_to(root)
        indent = "│   " * (len(rel.parts) - 1)
        prefix = "├── " if rel.parts else ""
        lines.append(f"{indent}{prefix}{path.name}{'/' if path.is_dir() else ''}")
    return "\n".join(lines)


# ────────────────────────────────────────────────────────────────────────────────
# MAIN
# ────────────────────────────────────────────────────────────────────────────────
def main() -> None:
    ap = argparse.ArgumentParser(description="Genera un snapshot limpio del proyecto.")
    ap.add_argument("root", nargs="?", default=".", help="Raíz del proyecto")
    ap.add_argument("-o", "--output", default="project_snapshot.txt", help="Archivo de salida")
    ap.add_argument("--ignore-dirs", help="Patrones coma-separados extra para ignorar")
    ap.add_argument("--ignore-files", help="Patrones coma-separados extra para ignorar")
    ap.add_argument("--only-ext", help="Extensiones permitidas (ej: .py,.md) ― vacío = todas")
    ap.add_argument("--max-size", type=int, default=DEFAULT_MAX_SIZE,
                    help=f"Tamaño máximo (bytes) de archivos a incluir (def {DEFAULT_MAX_SIZE})")
    args = ap.parse_args()

    filters = Filters(
        ignore_dirs=DEFAULT_IGNORE_DIRS + (args.ignore_dirs.split(",") if args.ignore_dirs else []),
        ignore_files=DEFAULT_IGNORE_FILES + (args.ignore_files.split(",") if args.ignore_files else []),
        only_ext=[e.lower() for e in (args.only_ext.split(",") if args.only_ext else DEFAULT_ONLY_EXT)],
        max_size=args.max_size,
    )

    root = Path(args.root).resolve()
    files = collect_files(root, filters)

    with open(args.output, "w", encoding="utf-8") as out:
        # 1. Árbol compacto
        out.write("# Estructura relevante del proyecto\n\n")
        out.write(build_tree(root, files) + "\n\n")

        # 2. Contenido de archivos
        out.write("# Contenido de archivos relevantes\n")
        for fp in files:
            rel = fp.relative_to(root)
            out.write("\n" + "-" * 80 + f"\n# {rel}\n\n")
            try:
                out.write(fp.read_text(encoding="utf-8", errors="replace") + "\n")
            except Exception as e:
                out.write(f"<<No se pudo leer este archivo: {e}>>\n")

    print(f"Snapshot listo → {args.output}")

if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
# pyproject.toml

[project]
name = "librorecomienda"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "alembic>=1.15.2",
    "fastapi>=0.115.12",
    "httpx>=0.28.1",
    "langchain>=0.3.24",
    "langgraph>=0.3.34",
    "openai>=1.76.0",
    "passlib[bcrypt]>=1.7.4",
    "psycopg2-binary>=2.9.10",
    "pydantic[email]>=2.11.3",
    "pydantic-settings>=2.9.1",
    "python-dotenv>=1.1.0",
    "sqlalchemy>=2.0.40",
    "streamlit>=1.44.1",
    "uvicorn[standard]>=0.34.2",
]

[dependency-groups]
dev = [
    "faker>=37.1.0",
]


--------------------------------------------------------------------------------
# scripts/generate_fake_data.py

# scripts/generate_fake_data.py
import random
import logging
import sys
import os # Necesario para la importación relativa si no se usa uv run
from faker import Faker
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError # Para capturar errores específicos

# --- Configuración del Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Añadir 'src' al PYTHONPATH (alternativa si no usas 'uv run') ---
# current_dir = os.path.dirname(os.path.abspath(__file__))
# project_root = os.path.dirname(current_dir)
# src_path = os.path.join(project_root, 'src')
# if src_path not in sys.path:
#     sys.path.insert(0, src_path)

# --- Importaciones del Proyecto ---
try:
    from librorecomienda.db.session import SessionLocal
    from librorecomienda.models.user import User
    from librorecomienda.models.book import Book
    from librorecomienda.models.review import Review
    from librorecomienda.schemas.user import UserCreate
    from librorecomienda.schemas.review import ReviewCreate
    from librorecomienda.crud.crud_user import create_user, get_user_by_email
    from librorecomienda.crud.crud_review import create_review
    MODELS_LOADED = True
    logger.info("Módulos del proyecto importados correctamente.")
except ImportError as e:
    logger.error(f"Error importando módulos: {e}.")
    logger.error("Asegúrate de haber ejecutado 'uv pip install -e .'")
    logger.error("Y que los archivos CRUD/Schemas/Models existen y son importables.")
    MODELS_LOADED = False
    sys.exit(1) # Salir si hay error crítico

# --- Constantes de Configuración ---
NUM_FAKE_USERS = 50        # Número de usuarios falsos a crear/verificar
MAX_REVIEWS_PER_USER = 15  # Máximo número de reseñas por cada usuario
MIN_REVIEWS_PER_USER = 2   # Mínimo número de reseñas por cada usuario
FAKE_PASSWORD = "password123" # Contraseña común para usuarios de prueba

# --- Inicializar Faker ---
fake = Faker(['es_ES', 'en_US']) # Múltiples locales para variedad de nombres/emails
logger.info("Instancia de Faker creada.")

# --- Función Principal ---
def generate_data():
    if not MODELS_LOADED:
        logger.error("No se pudieron cargar los módulos del proyecto. Abortando.")
        return

    logger.info("=============================================")
    logger.info(" Iniciando script de generación de datos falsos")
    logger.info("=============================================")

    db: Session | None = None
    created_user_ids = []
    book_ids = []

    try:
        logger.info("Abriendo sesión de base de datos...")
        db = SessionLocal()

        # 1. Crear o Verificar Usuarios Falsos
        logger.info(f"--- Fase 1: Creando/Verificando {NUM_FAKE_USERS} Usuarios Falsos ---")
        for i in range(NUM_FAKE_USERS):
            # --- Comenta o borra estas líneas ---
            # first_name = fake.first_name().lower().replace("'", "") # Evitar apóstrofes
            # last_name = fake.last_name().lower().replace("'", "")
            # domain = fake.domain_name()
            # Construir email más realista y único
            # fake_email = f"{first_name}.{last_name}{random.randint(1,999)}@{domain}"

            # --- Usa esta línea en su lugar ---
            fake_email = fake.safe_email()
            # Opcional: puedes añadir un log para ver qué genera
            # logger.debug(f"Generated safe email: {fake_email}")

            # Verificar si ya existe en la BD
            existing_user = get_user_by_email(db, email=fake_email)

            if not existing_user:
                user_in = UserCreate(email=fake_email, password=FAKE_PASSWORD)
                try:
                    # La función create_user ya hace commit y refresh
                    new_user = create_user(db=db, user=user_in)
                    created_user_ids.append(new_user.id)
                    logger.info(f"  ({i+1}/{NUM_FAKE_USERS}) Usuario Creado: {new_user.email} (ID: {new_user.id})")
                except IntegrityError: # Captura error si el email ya existe (raro por el número aleatorio)
                     db.rollback() # Deshacer la transacción fallida
                     logger.warning(f"  ({i+1}/{NUM_FAKE_USERS}) Error de integridad al crear {fake_email}, probablemente ya existe. Intentando obtenerlo...")
                     existing_user = get_user_by_email(db, email=fake_email)
                     if existing_user:
                          created_user_ids.append(existing_user.id)
                except Exception as e:
                    logger.error(f"  ({i+1}/{NUM_FAKE_USERS}) Error inesperado creando usuario {fake_email}: {e}")
                    db.rollback() # Deshacer si hay otro error
            else:
                # Si ya existe, simplemente usamos su ID
                logger.info(f"  ({i+1}/{NUM_FAKE_USERS}) Usuario Encontrado: {existing_user.email} (ID: {existing_user.id})")
                created_user_ids.append(existing_user.id)

        if not created_user_ids:
            logger.error("No se pudieron crear ni encontrar usuarios. Abortando generación de reseñas.")
            return

        logger.info(f"--- Fase 1 Completada: {len(created_user_ids)} IDs de usuario listos. ---")

        # 2. Obtener IDs de Libros Existentes
        logger.info("--- Fase 2: Obteniendo IDs de Libros Existentes ---")
        book_ids = [id_tuple[0] for id_tuple in db.query(Book.id).all()] # Extraer el ID de la tupla
        if not book_ids:
            logger.error("No hay libros en la base de datos. No se pueden generar reseñas.")
            return
        logger.info(f"Se encontraron {len(book_ids)} libros disponibles.")
        logger.info("--- Fase 2 Completada. ---")


        # 3. Crear Reseñas Falsas
        logger.info(f"--- Fase 3: Generando Reseñas Falsas ({MIN_REVIEWS_PER_USER}-{MAX_REVIEWS_PER_USER} por usuario) ---")
        total_reviews_added = 0
        processed_users = 0

        for user_id in created_user_ids:
            processed_users += 1
            # Determinar cuántas reseñas hará este usuario
            num_reviews_to_create = random.randint(MIN_REVIEWS_PER_USER, min(MAX_REVIEWS_PER_USER, len(book_ids)))

            # Seleccionar libros al azar para este usuario (sin repetición)
            # Asegurarse de que num_reviews_to_create no sea mayor que los libros disponibles
            actual_num_reviews = min(num_reviews_to_create, len(book_ids))
            if actual_num_reviews <= 0:
                continue # No hay libros para reseñar o num_reviews es 0
            selected_book_ids = random.sample(book_ids, actual_num_reviews)

            logger.info(f"  Usuario ID: {user_id} ({processed_users}/{len(created_user_ids)}) - Creando {actual_num_reviews} reseñas...")

            reviews_for_this_user = 0
            for book_id in selected_book_ids:
                # Generar datos de la reseña
                fake_rating = random.randint(1, 5)
                # Generar comentario con probabilidad (ej: 70% de las veces)
                fake_comment_text = fake.paragraph(nb_sentences=random.randint(1, 4)) if random.random() < 0.7 else None

                try:
                    # Crear el objeto schema
                    review_in = ReviewCreate(rating=fake_rating, comment=fake_comment_text)
                    # Llamar a la función CRUD (que hace commit)
                    create_review(db=db, review=review_in, user_id=user_id, book_id=book_id)
                    reviews_for_this_user += 1
                except IntegrityError as ie:
                     # Podría ocurrir si ya existe una reseña para este user/book y tienes UniqueConstraint
                     logger.warning(f"  Error de integridad al crear review para User {user_id}, Book {book_id}: {ie}. ¿Ya existe?")
                     db.rollback() # Deshacer transacción fallida
                except Exception as e:
                    logger.error(f"  Error inesperado creando review para User {user_id}, Book {book_id}: {e}")
                    db.rollback() # Deshacer transacción fallida

            if reviews_for_this_user > 0:
                 logger.info(f"  Usuario ID: {user_id} - Se crearon {reviews_for_this_user} reseñas.")
                 total_reviews_added += reviews_for_this_user
            # No es necesario commit aquí si create_review ya lo hace

        logger.info(f"--- Fase 3 Completada: Total reseñas falsas añadidas: {total_reviews_added} ---")

    except Exception as e:
        logger.exception(f"Error CRÍTICO durante la generación de datos: {e}")
        if db: db.rollback() # Intentar deshacer si hubo un error grave
    finally:
        if db:
            logger.info("Cerrando sesión de base de datos.")
            db.close()

# --- Punto de Entrada del Script ---
if __name__ == "__main__":
    generate_data()
    logger.info("============================================")
    logger.info(" Script de Generación de Datos Finalizado")
    logger.info("============================================")


--------------------------------------------------------------------------------
# scripts/populate_db.py

# scripts/populate_db.py
import logging
import sys
import os
import asyncio # Import asyncio

# --- Configuración del Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Añadir 'src' al PYTHONPATH (si es necesario) ---
# ... (código existente para añadir src al path) ...

# --- Importaciones del Proyecto ---
try:
    from sqlalchemy.orm import Session
    from librorecomienda.db.session import SessionLocal
    from librorecomienda.models.book import Book
    # --- Cambiar esta línea ---
    from librorecomienda.clients.google_books import search_books_google_api
    # -------------------------
    from librorecomienda.core.config import settings
    MODELS_LOADED = True
    logger.info("Módulos del proyecto importados correctamente.")
except ImportError as e:
    logger.error(f"Error importando módulos: {e}.")
    logger.error("Asegúrate de haber ejecutado 'uv pip install -e .' y 'uv pip install httpx'")
    MODELS_LOADED = False
    sys.exit(1)

# --- Constantes ---
SEARCH_QUERIES = [
    "python programming",
    "data science",
    "machine learning",
    "artificial intelligence",
    "software engineering best practices",
    "classic literature",
    "science fiction",
    "fantasy novels",
    "historical fiction",
    "biography"
]
MAX_RESULTS_PER_QUERY = 10 # Número de libros a intentar obtener por cada búsqueda

# --- Función para Poblar Libros ---
def populate_books(db: Session):
    if not MODELS_LOADED:
        logger.error("No se pudieron cargar los módulos. Abortando población.")
        return

    logger.info("--- Iniciando Población de Libros --- ")
    # No necesitamos instanciar un cliente
    total_books_added = 0

    for query in SEARCH_QUERIES:
        logger.info(f"Buscando libros para: '{query}'...")
        try:
            # --- Llamar a la función async usando asyncio.run() ---
            google_books_data = asyncio.run(
                search_books_google_api(query, max_results=MAX_RESULTS_PER_QUERY)
            )
            # -----------------------------------------------------

            if google_books_data is None: # La función devuelve None en caso de error
                logger.error(f"Error al buscar libros para '{query}'. Ver logs anteriores.")
                continue
            if not google_books_data:
                logger.warning(f"No se encontraron resultados para '{query}'.")
                continue

            logger.info(f"Se encontraron {len(google_books_data)} resultados para '{query}'. Procesando...")

            for item in google_books_data:
                volume_info = item.get('volumeInfo', {})

                title = volume_info.get('title')
                authors = volume_info.get('authors', [])
                author_str = ", ".join(authors) if authors else None
                description = volume_info.get('description')
                genre = volume_info.get('categories', [None])[0] # Tomar la primera categoría como género
                image_links = volume_info.get('imageLinks', {})
                cover_url = image_links.get('thumbnail') or image_links.get('smallThumbnail')

                # --- Extraer ISBN --- 
                industry_identifiers = volume_info.get('industryIdentifiers', [])
                isbn_13 = None
                isbn_10 = None
                for identifier in industry_identifiers:
                    id_type = identifier.get('type')
                    id_value = identifier.get('identifier')
                    if id_type == 'ISBN_13':
                        isbn_13 = id_value
                    elif id_type == 'ISBN_10':
                        isbn_10 = id_value

                # Priorizamos ISBN_13 si existe, si no, usamos ISBN_10
                book_isbn = isbn_13 if isbn_13 else isbn_10
                # Truncar si excede el límite de la BD (String(20))
                book_isbn = book_isbn[:20] if book_isbn else None
                # --------------------

                if not title:
                    logger.warning("Libro sin título encontrado, saltando.")
                    continue

                # Evitar duplicados
                exists = db.query(Book).filter(Book.title == title, Book.author == author_str).first()
                if exists:
                    logger.info(f"Libro ya existe (título/autor): '{title}'. Saltando.")
                    continue
                if book_isbn:
                     exists_isbn = db.query(Book).filter(Book.isbn == book_isbn).first()
                     if exists_isbn:
                           logger.info(f"Libro ya existe (ISBN): '{title}' [{book_isbn}]. Saltando.")
                           continue

                new_book = Book(
                    title=title[:255],
                    author=author_str[:255] if author_str else None,
                    genre=genre[:100] if genre else None,
                    description=description,
                    cover_image_url=cover_url[:512] if cover_url else None,
                    isbn=book_isbn
                )
                db.add(new_book)
                total_books_added += 1
                logger.info(f"  Añadido: '{new_book.title}' (ISBN: {new_book.isbn or 'N/A'})")

            # Hacer commit por lotes
            try:
                db.commit()
                logger.info(f"Commit realizado para libros de '{query}'.")
            except Exception as commit_exc:
                logger.error(f"Error haciendo commit para '{query}': {commit_exc}")
                db.rollback()

        except Exception as e:
            logger.exception(f"Error procesando la query '{query}': {e}")
            db.rollback()

    logger.info(f"--- Población de Libros Finalizada: {total_books_added} libros añadidos en total. ---")

# --- Punto de Entrada --- 
if __name__ == "__main__":
    db_session: Session | None = None
    try:
        logger.info("Abriendo sesión de base de datos para poblar...")
        db_session = SessionLocal()
        populate_books(db_session)
    except Exception as main_exc:
        logger.exception(f"Error CRÍTICO durante la población: {main_exc}")
    finally:
        if db_session:
            logger.info("Cerrando sesión de base de datos.")
            db_session.close()

--------------------------------------------------------------------------------
# src/__init__.py



--------------------------------------------------------------------------------
# streamlit_app/app.py

# streamlit_app/app.py
import streamlit as st
import pandas as pd
import time
from sqlalchemy.orm import Session

# Importaciones del proyecto (ajusta las rutas según tu estructura)
from librorecomienda.db.session import SessionLocal
from librorecomienda.models.book import Book
from librorecomienda.models.user import User # Asegúrate de importar User
from librorecomienda.schemas.user import UserCreate
from librorecomienda.schemas.review import ReviewCreate
from librorecomienda.crud import (
    create_user,
    get_user_by_email,
    create_review,
    get_reviews_for_book_with_user,
    get_users,
    soft_delete_review,
    get_all_reviews_admin, # <-- Import get_all_reviews_admin
)
from librorecomienda.core.security import verify_password, get_password_hash
from librorecomienda.core.config import settings # Importar settings

# --- Inicialización del Estado de Sesión ---
if 'logged_in' not in st.session_state:
    st.session_state['logged_in'] = False
    st.session_state['user_email'] = None
    st.session_state['user_id'] = None
    st.session_state['is_admin'] = False # <-- Añadir esta línea

# ... (resto de inicializaciones si las hay) ...

# --- Funciones Auxiliares (si las tienes) ---
# Ejemplo de función para cargar libros (si la tienes separada)
# Si no, la lógica estará directamente en la sección principal
@st.cache_data(ttl=3600) # Cachear por 1 hora
def load_books_from_db():
    db: Session | None = None
    try:
        db = SessionLocal()
        # Seleccionar todas las columnas necesarias, incluyendo isbn
        books_result = db.query(
            Book.id, Book.title, Book.author, Book.genre,
            Book.average_rating, Book.description,
            Book.cover_image_url,
            Book.isbn  # <-- Asegúrate que esta columna esté seleccionada
        ).order_by(Book.title).all() # Ordenar por título para consistencia

        # Convertir a un objeto más fácil de usar si prefieres (opcional)
        # import types # Necesitarías importar types
        # books_data = [
        #     types.SimpleNamespace(
        #         id=row.id, title=row.title, author=row.author, genre=row.genre,
        #         average_rating=row.average_rating, description=row.description,
        #         cover_image_url=row.cover_image_url,
        #         isbn=row.isbn # <-- Añade el isbn aquí también
        #     ) for row in books_result
        # ]
        # return books_data
        return books_result # Devolver directamente los resultados de SQLAlchemy (Row objects)
    except Exception as e:
        st.error(f"Error cargando libros desde la base de datos: {e}")
        return []
    finally:
        if db:
            db.close()

# --- Barra Lateral: Login / Registro / Logout ---
st.sidebar.title("Acceso")

if not st.session_state.get('logged_in', False):
    login_tab, register_tab = st.sidebar.tabs(["Iniciar Sesión", "Registrarse"])

    with login_tab:
        with st.form("login_form"):
            st.subheader("Iniciar Sesión")
            login_email = st.text_input("Email", key="login_email")
            login_password = st.text_input("Contraseña", type="password", key="login_password")
            login_submitted = st.form_submit_button("Entrar")

            if login_submitted:
                if not login_email or not login_password:
                    st.warning("Por favor, introduce email y contraseña.")
                else:
                    db: Session | None = None
                    try:
                        db = SessionLocal()
                        user = get_user_by_email(db, email=login_email)
                        if user and user.is_active and verify_password(login_password, user.hashed_password):
                            st.session_state['logged_in'] = True
                            st.session_state['user_email'] = user.email
                            st.session_state['user_id'] = user.id
                            # --- Añadir esta verificación ---
                            if user.email in settings.list_admin_emails:
                                st.session_state['is_admin'] = True
                                st.toast("Acceso de administrador concedido.", icon="🔑")
                            else:
                                st.session_state['is_admin'] = False
                            # --- Fin de la verificación ---
                            st.success("¡Login exitoso!")
                            time.sleep(1)
                            # No cerrar db aquí si se necesita más adelante en la misma ejecución
                            # db.close() # Mover close si es posible o gestionarlo al final
                            st.rerun()
                        else:
                            st.error("Email o contraseña incorrectos.")
                    except Exception as e:
                        st.error(f"Error durante el login: {e}")
                    finally:
                        if db:
                            db.close()

    with register_tab:
        with st.form("register_form"):
            st.subheader("Registrarse")
            register_email = st.text_input("Email", key="register_email")
            register_password = st.text_input("Contraseña", type="password", key="register_password")
            register_confirm_password = st.text_input("Confirmar Contraseña", type="password", key="register_confirm_password")
            register_submitted = st.form_submit_button("Registrar")

            if register_submitted:
                if not register_email or not register_password or not register_confirm_password:
                    st.warning("Por favor, rellena todos los campos.")
                elif register_password != register_confirm_password:
                    st.error("Las contraseñas no coinciden.")
                else:
                    db: Session | None = None
                    try:
                        db = SessionLocal()
                        existing_user = get_user_by_email(db, email=register_email)
                        if existing_user:
                            st.error("Este email ya está registrado.")
                        else:
                            user_in = UserCreate(email=register_email, password=register_password)
                            new_user = create_user(db=db, user=user_in)
                            st.success(f"¡Usuario {new_user.email} registrado con éxito! Ahora puedes iniciar sesión.")
                            time.sleep(2)
                            # Podrías hacer login automático aquí o simplemente limpiar
                    except Exception as e:
                        st.error(f"Error durante el registro: {e}")
                    finally:
                        if db:
                            db.close()
else:
    st.sidebar.write(f"Conectado como: {st.session_state['user_email']}")
    if st.session_state.get('is_admin', False):
        st.sidebar.markdown("**Rol:** Administrador 🔑")
    if st.sidebar.button("Cerrar Sesión"):
        st.session_state['logged_in'] = False
        st.session_state['user_email'] = None
        st.session_state['user_id'] = None
        st.session_state['is_admin'] = False # <-- Añadir esta línea
        st.success("Sesión cerrada.")
        time.sleep(1)
        st.rerun()

# --- Título Principal --- 
st.title("📚 LibroRecomienda")
st.write("Encuentra y comparte reseñas de tus libros favoritos.")

# --- Catálogo de Libros y Reseñas --- 
try:
    # Cargar libros (usando la función cacheada o directamente)
    # all_books = load_books_from_db() # Si usas la función auxiliar
    db_main = SessionLocal() # Abrir sesión si no usas la función auxiliar
    all_books = db_main.query(Book).order_by(Book.title).all() # Carga directa

    if not all_books:
        st.warning("No hay libros en la base de datos. Ejecuta `scripts/populate_db.py`.")
    else:
        st.header("Catálogo de Libros")
        # Aquí iría tu lógica de filtros y búsqueda si la tienes
        # Ejemplo simple de filtro (si lo implementas)
        # search_term = st.text_input("Buscar libro por título o autor")
        # filtered_books = [book for book in all_books if search_term.lower() in book.title.lower() or (book.author and search_term.lower() in book.author.lower())] if search_term else all_books
        filtered_books = all_books # Sin filtro por ahora

        for book in filtered_books:
            # Usar book.id como parte de la clave del expander para unicidad
            # Se elimina el argumento 'key' para compatibilidad con versiones anteriores de Streamlit
            with st.expander(f"{book.title} ({book.author or 'Autor Desconocido'})"):
                col1, col2 = st.columns([1, 3])
                with col1:
                    if book.cover_image_url:
                        # Añadir manejo de errores para la imagen
                        try:
                            st.image(book.cover_image_url, width=150)
                        except Exception as img_e:
                            st.caption(f"Error cargando portada: {img_e}")
                    else:
                        st.caption("Sin portada")
                with col2:
                    st.subheader(f"{book.title}")
                    st.write(f"**Autor:** {book.author or 'Desconocido'}")
                    # st.write(f"**Año:** {book.publication_year}") # Ya comentado/eliminado
                    # --- Mostrar ISBN si existe --- 
                    if book.isbn:
                        st.write(f"**ISBN:** {book.isbn}")
                    # -------------------------------
                    st.write(f"**Género:** {book.genre or 'Desconocido'}")
                    # Mostrar descripción si existe
                    if book.description:
                        st.caption(f"Descripción: {book.description[:200]}...") # Mostrar solo una parte

                # --- Sección de Reseñas ---
                st.markdown("#### Reseñas de otros usuarios")
                # Asegurarse de pasar la sesión correcta a las funciones CRUD
                # Use the updated function that returns Row objects (Review, User.email)
                reviews_data = get_reviews_for_book_with_user(db=db_main, book_id=book.id)
                if reviews_data:
                    # reviews_data is a list of Row objects, access attributes by name
                    for review_row in reviews_data:
                        review = review_row.Review # Access the Review object
                        user_email = review_row.email # Access the user's email

                        # Display review details
                        st.markdown(f"**{user_email or 'Usuario Desconocido'}** ({'⭐'*review.rating}):")
                        if review.comment:
                            st.markdown(f"> *{review.comment}*")
                        st.caption(f"_{review.created_at.strftime('%Y-%m-%d %H:%M') if review.created_at else 'Fecha desconocida'}_")

                        # --- Botón Borrar (si es mi reseña y estoy logueado) ---
                        # Compara el user_id de la reseña con el user_id en session_state
                        if st.session_state.get('logged_in', False) and review.user_id == st.session_state.get('user_id'):
                            # Usamos un key único para cada botón de borrar
                            if st.button("🗑️ Borrar mi reseña", key=f"delete_review_{review.id}", type="secondary"):
                                # Optional: Add a confirmation step if desired
                                # st.warning("¿Estás seguro?")
                                # if st.button("Confirmar Borrado", key=f"confirm_delete_{review.id}"):
                                delete_db: Session | None = None
                                try:
                                    delete_db = SessionLocal()
                                    # Llamar a la función CRUD de borrado lógico
                                    success = soft_delete_review(
                                        db=delete_db,
                                        review_id=review.id, # Pasar el ID de la reseña actual
                                        requesting_user_id=st.session_state['user_id']
                                    )
                                    if success:
                                        st.toast("Reseña borrada.", icon="🗑️")
                                        # Limpiar caché si usas @st.cache_data en load_books_from_db
                                        # O simplemente limpiar la caché general de datos si afecta a las reseñas
                                        st.cache_data.clear()
                                        time.sleep(1) # Pausa para ver el toast
                                        st.rerun() # Refrescar la página
                                    else:
                                        # Podría ser que no se encontró o no tenía permiso (soft_delete_review ya loguea el error)
                                        st.warning("No se pudo borrar la reseña (quizás ya estaba borrada o hubo un problema).")
                                except Exception as e_del:
                                    st.error(f"Error al intentar borrar: {e_del}")
                                finally:
                                    if delete_db:
                                        delete_db.close()
                        st.markdown("---") # Separator between reviews
                else:
                    st.caption("Todavía no hay reseñas para este libro.")

                # --- Añadir Reseña (Solo si está logueado) ---
                if st.session_state.get('logged_in', False):
                    st.markdown("#### Añade tu reseña")
                    # Usar book.id en la clave del formulario para unicidad
                    with st.form(key=f"review_form_{book.id}"):
                        rating = st.slider("Puntuación", 1, 5, 3, key=f"rating_{book.id}")
                        comment = st.text_area("Comentario (opcional)", key=f"comment_{book.id}")
                        submit_review = st.form_submit_button("Enviar Reseña")

                        if submit_review:
                            review_in = ReviewCreate(rating=rating, comment=comment)
                            try:
                                # Asegurarse de pasar la sesión correcta
                                create_review(db=db_main, review=review_in, user_id=st.session_state['user_id'], book_id=book.id)
                                st.success("¡Reseña añadida con éxito!")
                                time.sleep(1)
                                # Limpiar cache si usas @st.cache_data en load_books_from_db
                                # load_books_from_db.clear()
                                st.rerun() # Recargar para ver la nueva reseña
                            except Exception as e:
                                st.error(f"Error al añadir la reseña: {e}")
                                # db_main.rollback() # Rollback si es necesario

except Exception as e:
    st.error(f"Error cargando los libros o reseñas: {e}")
    # Asegurarse de cerrar la sesión si se abrió aquí
    if 'db_main' in locals() and db_main:
        db_main.close()
finally:
    # Asegurarse de cerrar la sesión si se abrió en el bloque try principal
    if 'db_main' in locals() and db_main:
        db_main.close()


# --- Sección de Administración (Solo visible para admins) ---
if st.session_state.get('is_admin'):
    st.sidebar.divider()
    st.sidebar.header("Panel de Administración")
    admin_option = st.sidebar.radio("Selecciona una vista:", ["Gestión de Usuarios", "Gestión de Reseñas"], key="admin_view")

    with db:
        if admin_option == "Gestión de Usuarios":
            st.subheader("Gestión de Usuarios")
            users_data = get_users(db) # Use directly
            if users_data:
                # Crear un DataFrame de Pandas para mostrar en tabla
                # Usamos los nombres de columna que seleccionamos en get_users
                # Asegúrate de que pandas está instalado: uv pip install pandas
                try:
                    import pandas as pd
                    df_users = pd.DataFrame(users_data, columns=['ID', 'Email', 'Activo', 'Creado', 'Actualizado'])
                    st.dataframe(df_users, use_container_width=True)
                except ImportError:
                    st.error("La librería 'pandas' no está instalada. Por favor, ejecute `uv pip install pandas`.")
                    st.write("Datos de usuarios (sin formato tabla):")
                    st.write(users_data) # Mostrar datos crudos si pandas no está
            else:
                st.write("No hay usuarios registrados.")

        elif admin_option == "Gestión de Reseñas": # <-- Use elif
            st.subheader("Gestión de Reseñas")
            reviews_admin_data = get_all_reviews_admin(db) # Use directly
            if reviews_admin_data:
                reviews_list = []
                for review, user_email, book_title in reviews_admin_data:
                    reviews_list.append(
                        {
                            "ID Reseña": review.id,
                            "Libro": book_title,
                            "Usuario": user_email,
                            "Puntuación": review.rating,
                            "Comentario": review.comment,
                            "Fecha": review.created_at.strftime("%Y-%m-%d %H:%M"),
                            "Estado": "BORRADO" if review.is_deleted else "Activo",
                        }
                    )
                reviews_df = pd.DataFrame(reviews_list)
                st.dataframe(reviews_df, use_container_width=True)
            else:
                st.write("No hay reseñas para mostrar.")

# --- Fin Panel de Administración ---

st.divider()

--------------------------------------------------------------------------------
# tests/__init__.py


